{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to:\n",
    "- Load the snapshots of X and BlueSky data\n",
    "- Format them into threads (give replies/quotes their necessary context)\n",
    "- Filter by the politician-focused keyword lists\n",
    "- Export for narrative extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "from itertools import product \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "From Google Drive snapshots  \n",
    "Only unzipped bluesky-2025-02 and -03 (not -01), since we only have the last four days of -02 for x for comparison.\n",
    "\n",
    "100+ GB of BlueSky data. So just loading 14 days starting from first available X data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loads_jsonl(data: str):\n",
    "    return [json.loads(line) for line in data.split('\\n')]\n",
    "\n",
    "def escape_newlines_in_json(json_str):\n",
    "    return json_str.replace('\\n', '\\\\n')\n",
    "\n",
    "def load_jsonl_str(json_str):\n",
    "    # Split into newlines and load as list of Python dicts\n",
    "    json_chunks = re.split(r'\\n(?=\\{)', json_str.strip())\n",
    "\n",
    "    data_objects = []\n",
    "    for chunk in json_chunks:\n",
    "        escaped_chunk = escape_newlines_in_json(chunk)\n",
    "        try:\n",
    "            obj = json.loads(escaped_chunk)\n",
    "            data_objects.append(obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Error decoding a chunk:\", e)\n",
    "    \n",
    "    return data_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data from specified buckets and dates\n",
    "data_path = './data/snapshots'\n",
    "buckets = [\n",
    "    'bluesky',\n",
    "    'x',\n",
    "]\n",
    "years = ['2025']\n",
    "months_days = {\n",
    "    '02': ['26', '27'],\n",
    "    '03': [],\n",
    "    # f'{day:02d}' for day in range(1, 2)\n",
    "}\n",
    "hours = [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading bluesky/2025-02-26/03: 100%|██████████| 12/12 [00:00<00:00, 48.48it/s]\n",
      "Loading bluesky/2025-02-26/04: 100%|██████████| 12/12 [00:00<00:00, 69.57it/s]\n",
      "Loading bluesky/2025-02-26/05: 100%|██████████| 12/12 [00:00<00:00, 102.04it/s]\n",
      "Loading bluesky/2025-02-26/02: 100%|██████████| 12/12 [00:00<00:00, 51.36it/s]\n",
      "Loading bluesky/2025-02-26/20: 100%|██████████| 26/26 [00:08<00:00,  3.15it/s]\n",
      "Loading bluesky/2025-02-26/18: 100%|██████████| 11/11 [00:02<00:00,  5.39it/s]\n",
      "Loading bluesky/2025-02-26/11: 100%|██████████| 12/12 [00:00<00:00, 121.43it/s]\n",
      "Loading bluesky/2025-02-26/16: 100%|██████████| 12/12 [00:00<00:00, 107.63it/s]\n",
      "Loading bluesky/2025-02-26/17: 100%|██████████| 6/6 [00:00<00:00, 46.59it/s]\n",
      "Loading bluesky/2025-02-26/10: 100%|██████████| 12/12 [00:00<00:00, 69.15it/s]\n",
      "Loading bluesky/2025-02-26/19: 100%|██████████| 10/10 [00:02<00:00,  3.84it/s]\n",
      "Loading bluesky/2025-02-26/21: 100%|██████████| 19/19 [00:06<00:00,  3.10it/s]\n",
      "Loading bluesky/2025-02-26/07: 100%|██████████| 12/12 [00:00<00:00, 89.39it/s]\n",
      "Loading bluesky/2025-02-26/00: 100%|██████████| 12/12 [00:00<00:00, 45.09it/s]\n",
      "Loading bluesky/2025-02-26/09: 100%|██████████| 12/12 [00:00<00:00, 137.77it/s]\n",
      "Loading bluesky/2025-02-26/08: 100%|██████████| 12/12 [00:00<00:00, 95.00it/s]\n",
      "Loading bluesky/2025-02-26/01: 100%|██████████| 12/12 [00:00<00:00, 46.22it/s]\n",
      "Loading bluesky/2025-02-26/06: 100%|██████████| 11/11 [00:00<00:00, 87.06it/s]\n",
      "Loading bluesky/2025-02-26/23: 100%|██████████| 25/25 [00:07<00:00,  3.30it/s]\n",
      "Loading bluesky/2025-02-26/15: 100%|██████████| 12/12 [00:00<00:00, 53.60it/s]\n",
      "Loading bluesky/2025-02-26/12: 100%|██████████| 11/11 [00:00<00:00, 53.80it/s]\n",
      "Loading bluesky/2025-02-26/13: 100%|██████████| 12/12 [00:00<00:00, 35.12it/s]\n",
      "Loading bluesky/2025-02-26/14: 100%|██████████| 12/12 [00:00<00:00, 41.04it/s]\n",
      "Loading bluesky/2025-02-26/22: 100%|██████████| 25/25 [00:09<00:00,  2.51it/s]\n",
      "Loading bluesky/2025-02-27/03: 100%|██████████| 25/25 [00:07<00:00,  3.19it/s]\n",
      "Loading bluesky/2025-02-27/04: 100%|██████████| 26/26 [00:10<00:00,  2.60it/s]\n",
      "Loading bluesky/2025-02-27/05: 100%|██████████| 25/25 [00:04<00:00,  5.61it/s]\n",
      "Loading bluesky/2025-02-27/02: 100%|██████████| 24/24 [00:09<00:00,  2.45it/s]\n",
      "Loading bluesky/2025-02-27/20: 100%|██████████| 25/25 [00:10<00:00,  2.33it/s]\n",
      "Loading bluesky/2025-02-27/18: 100%|██████████| 25/25 [00:04<00:00,  5.09it/s]\n",
      "Loading bluesky/2025-02-27/11: 100%|██████████| 27/27 [00:12<00:00,  2.11it/s]\n",
      "Loading bluesky/2025-02-27/16: 100%|██████████| 26/26 [00:04<00:00,  5.34it/s]\n",
      "Loading bluesky/2025-02-27/17: 100%|██████████| 27/27 [00:14<00:00,  1.89it/s]\n",
      "Loading bluesky/2025-02-27/10: 100%|██████████| 27/27 [00:05<00:00,  5.08it/s]\n",
      "Loading bluesky/2025-02-27/19: 100%|██████████| 27/27 [00:14<00:00,  1.82it/s]\n",
      "Loading bluesky/2025-02-27/21: 100%|██████████| 26/26 [00:05<00:00,  4.50it/s]\n",
      "Loading bluesky/2025-02-27/07: 100%|██████████| 25/25 [00:04<00:00,  5.41it/s]\n",
      "Loading bluesky/2025-02-27/00: 100%|██████████| 25/25 [00:23<00:00,  1.08it/s]\n",
      "Loading bluesky/2025-02-27/09: 100%|██████████| 27/27 [00:05<00:00,  5.24it/s]\n",
      "Loading bluesky/2025-02-27/08: 100%|██████████| 26/26 [00:04<00:00,  5.64it/s]\n",
      "Loading bluesky/2025-02-27/01: 100%|██████████| 26/26 [00:22<00:00,  1.16it/s]\n",
      "Loading bluesky/2025-02-27/06: 100%|██████████| 25/25 [00:05<00:00,  4.93it/s]\n",
      "Loading bluesky/2025-02-27/23: 100%|██████████| 26/26 [00:04<00:00,  5.27it/s]\n",
      "Loading bluesky/2025-02-27/15: 100%|██████████| 26/26 [00:05<00:00,  4.75it/s]\n",
      "Loading bluesky/2025-02-27/12: 100%|██████████| 27/27 [00:19<00:00,  1.38it/s]\n",
      "Loading bluesky/2025-02-27/13: 100%|██████████| 27/27 [00:04<00:00,  5.61it/s]\n",
      "Loading bluesky/2025-02-27/14: 100%|██████████| 26/26 [00:05<00:00,  4.77it/s]\n",
      "Loading bluesky/2025-02-27/22: 100%|██████████| 26/26 [00:04<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bluesky Dataframe shape: (8105175, 294)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading x/2025-02-26/03: 100%|██████████| 11/11 [00:00<00:00, 17.84it/s]\n",
      "Loading x/2025-02-26/04: 100%|██████████| 12/12 [00:00<00:00, 28.66it/s]\n",
      "Loading x/2025-02-26/05: 100%|██████████| 12/12 [00:00<00:00, 34.74it/s]\n",
      "Loading x/2025-02-26/02: 100%|██████████| 12/12 [00:00<00:00, 15.30it/s]\n",
      "Loading x/2025-02-26/20: 100%|██████████| 11/11 [00:00<00:00, 17.63it/s]\n",
      "Loading x/2025-02-26/18: 100%|██████████| 12/12 [00:00<00:00, 18.44it/s]\n",
      "Loading x/2025-02-26/11: 100%|██████████| 11/11 [00:00<00:00, 28.62it/s]\n",
      "Loading x/2025-02-26/16: 100%|██████████| 12/12 [00:00<00:00, 20.56it/s]\n",
      "Loading x/2025-02-26/17: 100%|██████████| 12/12 [00:00<00:00, 19.34it/s]\n",
      "Loading x/2025-02-26/10: 100%|██████████| 11/11 [00:00<00:00, 48.95it/s]\n",
      "Loading x/2025-02-26/19: 100%|██████████| 12/12 [00:02<00:00,  5.49it/s]\n",
      "Loading x/2025-02-26/21: 100%|██████████| 12/12 [00:00<00:00, 17.18it/s]\n",
      "Loading x/2025-02-26/07: 100%|██████████| 12/12 [00:00<00:00, 43.43it/s]\n",
      "Loading x/2025-02-26/00: 100%|██████████| 12/12 [00:00<00:00, 22.14it/s]\n",
      "Loading x/2025-02-26/09: 100%|██████████| 12/12 [00:00<00:00, 49.77it/s]\n",
      "Loading x/2025-02-26/08: 100%|██████████| 11/11 [00:00<00:00, 13.97it/s]\n",
      "Loading x/2025-02-26/01: 100%|██████████| 12/12 [00:00<00:00, 23.49it/s]\n",
      "Loading x/2025-02-26/06: 100%|██████████| 11/11 [00:00<00:00, 48.52it/s]\n",
      "Loading x/2025-02-26/23: 100%|██████████| 12/12 [00:00<00:00, 20.98it/s]\n",
      "Loading x/2025-02-26/15: 100%|██████████| 11/11 [00:00<00:00, 20.89it/s]\n",
      "Loading x/2025-02-26/12: 100%|██████████| 12/12 [00:00<00:00, 30.56it/s]\n",
      "Loading x/2025-02-26/13: 100%|██████████| 12/12 [00:01<00:00, 11.16it/s]\n",
      "Loading x/2025-02-26/14: 100%|██████████| 12/12 [00:00<00:00, 36.00it/s]\n",
      "Loading x/2025-02-26/22: 100%|██████████| 12/12 [00:00<00:00, 24.05it/s]\n",
      "Loading x/2025-02-27/03: 100%|██████████| 12/12 [00:00<00:00, 21.49it/s]\n",
      "Loading x/2025-02-27/04: 100%|██████████| 12/12 [00:00<00:00, 33.48it/s]\n",
      "Loading x/2025-02-27/05: 100%|██████████| 12/12 [00:00<00:00, 46.43it/s]\n",
      "Loading x/2025-02-27/02: 100%|██████████| 11/11 [00:00<00:00, 26.60it/s]\n",
      "Loading x/2025-02-27/20: 100%|██████████| 11/11 [00:01<00:00,  7.85it/s]\n",
      "Loading x/2025-02-27/18: 100%|██████████| 12/12 [00:00<00:00, 17.41it/s]\n",
      "Loading x/2025-02-27/11: 100%|██████████| 11/11 [00:00<00:00, 38.50it/s]\n",
      "Loading x/2025-02-27/16: 100%|██████████| 12/12 [00:00<00:00, 16.52it/s]\n",
      "Loading x/2025-02-27/17: 100%|██████████| 12/12 [00:00<00:00, 16.63it/s]\n",
      "Loading x/2025-02-27/10: 100%|██████████| 11/11 [00:00<00:00, 55.25it/s]\n",
      "Loading x/2025-02-27/19: 100%|██████████| 12/12 [00:00<00:00, 18.50it/s]\n",
      "Loading x/2025-02-27/21: 100%|██████████| 12/12 [00:00<00:00, 16.81it/s]\n",
      "Loading x/2025-02-27/07: 100%|██████████| 11/11 [00:00<00:00, 16.66it/s]\n",
      "Loading x/2025-02-27/00: 100%|██████████| 12/12 [00:02<00:00,  5.54it/s]\n",
      "Loading x/2025-02-27/09: 100%|██████████| 11/11 [00:00<00:00, 35.32it/s]\n",
      "Loading x/2025-02-27/08: 100%|██████████| 11/11 [00:00<00:00, 84.93it/s]\n",
      "Loading x/2025-02-27/01: 100%|██████████| 12/12 [00:00<00:00, 18.89it/s]\n",
      "Loading x/2025-02-27/06: 100%|██████████| 11/11 [00:00<00:00, 61.15it/s]\n",
      "Loading x/2025-02-27/23: 100%|██████████| 12/12 [00:00<00:00, 17.70it/s]\n",
      "Loading x/2025-02-27/15: 100%|██████████| 11/11 [00:00<00:00, 21.42it/s]\n",
      "Loading x/2025-02-27/12: 100%|██████████| 12/12 [00:00<00:00, 41.23it/s]\n",
      "Loading x/2025-02-27/13: 100%|██████████| 12/12 [00:00<00:00, 29.40it/s]\n",
      "Loading x/2025-02-27/14: 100%|██████████| 12/12 [00:00<00:00, 21.42it/s]\n",
      "Loading x/2025-02-27/22: 100%|██████████| 12/12 [00:00<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Dataframe shape: (29401, 49)\n"
     ]
    }
   ],
   "source": [
    "# Load data and directly construct records to save memory\n",
    "dataframes = {}\n",
    "for bucket in buckets:\n",
    "    records = []\n",
    "    for year, month in product(years, months_days.keys()):\n",
    "        base_path = os.path.join(data_path, bucket, f\"{bucket}-{year}-{month}\")\n",
    "        if not os.path.exists(base_path):\n",
    "            continue\n",
    "\n",
    "        for day in months_days[month]:\n",
    "            day_path = os.path.join(base_path, day)\n",
    "            if not os.path.isdir(day_path):\n",
    "                continue\n",
    "\n",
    "            available_hours = os.listdir(day_path) if -1 in hours else hours\n",
    "            for hour in available_hours:\n",
    "                hour_path = os.path.join(day_path, hour)\n",
    "                if not os.path.isdir(hour_path):\n",
    "                    continue\n",
    "\n",
    "                files = [f for f in os.listdir(hour_path)]\n",
    "                for filename in tqdm(files, desc=f\"Loading {bucket}/{year}-{month}-{day}/{hour}\"):\n",
    "                    file_path = os.path.join(hour_path, filename)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        data = file.read()\n",
    "                    data_list = loads_jsonl(data)\n",
    "                    # data_list = load_jsonl_str(data)\n",
    "                    for idx, data in enumerate(data_list):\n",
    "                        records.append({\"bucket\": bucket, \"file\": file_path, \"data_idx\": idx, **data})\n",
    "                    del data\n",
    "                    del data_list\n",
    "    \n",
    "    dataframes[bucket] = pd.json_normalize(records)\n",
    "    print(f'{bucket.capitalize()} Dataframe shape:', dataframes[bucket].shape)\n",
    "    del records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify thread roots (original posts)\n",
    "def identify_thread_roots_bluesky(df):\n",
    "    return df[df['commit.record.reply.parent.uri'].isna() & df['commit.record.reply.root.uri'].isna()]\n",
    "\n",
    "def identify_thread_roots_x(df):\n",
    "    return df[df['data.referenced_tweets'].isna()]\n",
    "\n",
    "# Filter original posts based on keywords or authors\n",
    "def filter_original_posts(df, text_col, keywords):\n",
    "    pattern = '|'.join(keywords)\n",
    "    return df[\n",
    "        df[text_col].str.contains(pattern, case=False, na=False, regex=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keywords\n",
    "k_path = './data/keywords/bluesky_keywords_politicians.txt'\n",
    "with open(k_path, 'r') as f:\n",
    "    keywords_bluesky = f.readlines()\n",
    "keywords_bluesky = [k.strip() for k in keywords_bluesky]\n",
    "\n",
    "k_path = './data/keywords/x_keywords_politicians.txt'\n",
    "with open(k_path, 'r') as f:\n",
    "    keywords_x = f.readlines()\n",
    "keywords_x = [k.strip() for k in keywords_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bluesky:\n",
    "    - Text:\n",
    "        - commit.record.text\n",
    "- X\n",
    "    - Text:\n",
    "        - data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get root posts\n",
    "bluesky_roots = identify_thread_roots_bluesky(dataframes['bluesky'])\n",
    "x_roots = identify_thread_roots_x(dataframes['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered BlueSky roots shape: (2391, 294)\n",
      "Filtered X roots shape: (303, 49)\n"
     ]
    }
   ],
   "source": [
    "# Filtered roots based on keywords and authors\n",
    "filtered_bluesky_roots = filter_original_posts(\n",
    "    bluesky_roots, 'commit.record.text', keywords_bluesky\n",
    ")\n",
    "filtered_x_roots = filter_original_posts(\n",
    "    x_roots, 'data.text', keywords_x\n",
    ")\n",
    "print('Filtered BlueSky roots shape:', filtered_bluesky_roots.shape)\n",
    "print('Filtered X roots shape:', filtered_x_roots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And those goals serve the Anointed, not us plebs. Imagine, all the poverty for 1.6% of emissions, even if you believe it\n",
      "\n",
      "The contrast between us and the US will eventually resemble North vs. South Korea\n",
      "FUCK YOU @MarkJCarney \n",
      "ANOINTED @liberal_party \n",
      "@CBC\n",
      "https://t.co/PkKtPiqImF\n"
     ]
    }
   ],
   "source": [
    "print(filtered_x_roots['data.text'].sample(1).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002958348897349995"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_bluesky_roots.shape[0] / bluesky_roots.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17647058823529413"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_x_roots.shape[0] / x_roots.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to: ./data/to_annotate/df_x_sample_filtered_20250226_20250227.xlsx\n"
     ]
    }
   ],
   "source": [
    "cols_keep = [\n",
    "    'bucket', 'file', 'data_idx', 'matching_rules',\n",
    "    'data.author_id', 'data.conversation_id',\n",
    "    'data.text', 'data.referenced_tweets', 'includes.media',\n",
    "]\n",
    "fname = f'./data/to_annotate/df_x_sample_filtered_20250226_20250227.xlsx'\n",
    "filtered_x_roots[cols_keep].to_excel(fname, index=False)\n",
    "print('Wrote to:', fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to: ./data/to_annotate/df_bluesky_sample_filtered_20250226_20250227.xlsx\n"
     ]
    }
   ],
   "source": [
    "cols_keep = [\n",
    "    'bucket', 'file', 'data_idx',\n",
    "    'commit.record.reply.parent.uri', 'commit.record.reply.root.uri',\n",
    "    'commit.record.text', 'commit.record.title', 'commit.record.embed.external.uri',\n",
    "]\n",
    "fname = f'./data/to_annotate/df_bluesky_sample_filtered_20250226_20250227.xlsx'\n",
    "filtered_bluesky_roots[cols_keep].to_excel(fname, index=False)\n",
    "print('Wrote to:', fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO Extract with Open AI (zero-shot and with few-shot prompting)\n",
    "- Get similarity score performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexdata-kernel",
   "language": "python",
   "name": "complexdata-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
