{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform narrative extraction on larger set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "from itertools import product \n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import tiktoken\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_openai = os.getenv('KEY_OPENAI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interactions_x : (385208, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3675906/1981963069.py:5: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  globals()[new_object_name] = pd.read_csv(os.path.join(data_dir, fname))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts_x : (368354, 34)\n",
      "users_x : (17933, 24)\n",
      "interactions_bluesky : (460458, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3675906/1981963069.py:5: DtypeWarning: Columns (14,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  globals()[new_object_name] = pd.read_csv(os.path.join(data_dir, fname))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts_bluesky : (349796, 27)\n",
      "users_bluesky : (274495, 10)\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/consolidated'\n",
    "fnames = os.listdir(data_dir)\n",
    "for fname in fnames:\n",
    "    new_object_name = '_'.join(fname.split('_')[:2])\n",
    "    globals()[new_object_name] = pd.read_csv(os.path.join(data_dir, fname))\n",
    "    print(new_object_name, ':', globals()[new_object_name].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(305, 19)\n",
      "(52, 19)\n"
     ]
    }
   ],
   "source": [
    "fname = f'./data/annotated/df_x_sample_filtered_20250226_20250227_annotated.xlsx'\n",
    "df_x = pd.read_excel(fname)\n",
    "print(df_x.shape)\n",
    "df_x = df_x[:52]\n",
    "print(df_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10518, 18)\n",
      "(114, 18)\n"
     ]
    }
   ],
   "source": [
    "fname = f'./data/annotated/df_bluesky_sample_filtered_20250226_20250227_annotated.xlsx'\n",
    "df_bluesky = pd.read_excel(fname)\n",
    "print(df_bluesky.shape)\n",
    "df_bluesky = df_bluesky[:114]\n",
    "print(df_bluesky.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=key_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-4o-mini-2024-07-18'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define json schema for extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarrativeEvent(BaseModel):\n",
    "    agent: str\n",
    "    agent_norm: str\n",
    "    action_or_event: str\n",
    "    action_or_event_norm: str\n",
    "    object: str\n",
    "    object_norm: str\n",
    "    narrative: str\n",
    "\n",
    "class NarrativeExtraction(BaseModel):\n",
    "    events: List[NarrativeEvent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'agent', 'agent_norm',\n",
    "    'action_or_event', 'action_or_event_norm',\n",
    "    'object', 'object_norm',\n",
    "    'narrative', 'sentiment',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_if_list(value):\n",
    "    if isinstance(value, (list, tuple)) or (hasattr(value, 'ndim') and value.ndim == 1):\n",
    "        return ', '.join(str(x) for x in value)\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    return value\n",
    "\n",
    "def is_list_like(value):\n",
    "    return isinstance(value, (list, tuple)) or (hasattr(value, 'ndim') and value.ndim == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_examples = df_x[df_x['good_example'] == 1].groupby(['data.text']).agg({\n",
    "    col: list for col in cols\n",
    "}).reset_index()\n",
    "\n",
    "fewshot_inputs_x = []\n",
    "fewshot_outputs_x = []\n",
    "for index, row in df_x_examples.iterrows():\n",
    "    fewshot_inputs_x.append(row['data.text'])\n",
    "    prepared = {}\n",
    "    max_len = 1  # minimum 1 event per row\n",
    "    \n",
    "    # Process each column value. If it's missing, assign a default list;\n",
    "    # if list-like, use it as is; otherwise, wrap it in a one-element list.\n",
    "    for col in cols:\n",
    "        val = row[col]\n",
    "        if is_list_like(val):\n",
    "            prepared[col] = list(val)\n",
    "            if len(prepared[col]) > max_len:\n",
    "                max_len = len(prepared[col])\n",
    "        else:\n",
    "            prepared[col] = [val]\n",
    "    \n",
    "    # Pad any lists shorter than max_len with empty defaults\n",
    "    for col in cols:\n",
    "        if len(prepared[col]) < max_len:\n",
    "            pad_val = 0 if col == \"sentiment\" else \"\"\n",
    "            prepared[col].extend([pad_val] * (max_len - len(prepared[col])))\n",
    "    \n",
    "    # Create one event per index position and append to a list for that row\n",
    "    events = []\n",
    "    for i in range(max_len):\n",
    "        event = {\n",
    "            \"agent\": prepared[\"agent\"][i] if not pd.isna(prepared[\"agent\"][i]) else None,\n",
    "            \"agent_norm\": prepared[\"agent_norm\"][i] if not pd.isna(prepared[\"agent_norm\"][i]) else None,\n",
    "            \"action_or_event\": prepared[\"action_or_event\"][i] if not pd.isna(prepared[\"action_or_event\"][i]) else None,\n",
    "            \"action_or_event_norm\": prepared[\"action_or_event_norm\"][i] if not pd.isna(prepared[\"action_or_event_norm\"][i]) else None,\n",
    "            \"object\": prepared[\"object\"][i] if not pd.isna(prepared[\"object\"][i]) else None,\n",
    "            \"object_norm\": prepared[\"object_norm\"][i] if not pd.isna(prepared[\"object_norm\"][i]) else None,\n",
    "            \"narrative\": prepared[\"narrative\"][i] if not pd.isna(prepared[\"narrative\"][i]) else None,\n",
    "            \"sentiment\": prepared[\"sentiment\"][i] if not pd.isna(prepared[\"sentiment\"][i]) else None,\n",
    "        }\n",
    "        events.append(event)\n",
    "    \n",
    "    fewshot_outputs_x.append(events)\n",
    "fewshot_outputs_x[1] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bluesky_examples = df_bluesky[df_bluesky['good_example'] == 1].groupby(['commit.record.text']).agg({\n",
    "    col: list for col in cols\n",
    "}).reset_index()\n",
    "\n",
    "fewshot_inputs_bluesky = []\n",
    "fewshot_outputs_bluesky = []\n",
    "for index, row in df_bluesky_examples.iterrows():\n",
    "    fewshot_inputs_bluesky.append(row['commit.record.text'])\n",
    "    prepared = {}\n",
    "    max_len = 1  # minimum 1 event per row\n",
    "    \n",
    "    # Process each column value. If it's missing, assign a default list;\n",
    "    # if list-like, use it as is; otherwise, wrap it in a one-element list.\n",
    "    for col in cols:\n",
    "        val = row[col]\n",
    "        if is_list_like(val):\n",
    "            prepared[col] = list(val)\n",
    "            if len(prepared[col]) > max_len:\n",
    "                max_len = len(prepared[col])\n",
    "        else:\n",
    "            prepared[col] = [val]\n",
    "    \n",
    "    # Pad any lists shorter than max_len with empty defaults\n",
    "    for col in cols:\n",
    "        if len(prepared[col]) < max_len:\n",
    "            pad_val = 0 if col == \"sentiment\" else \"\"\n",
    "            prepared[col].extend([pad_val] * (max_len - len(prepared[col])))\n",
    "    \n",
    "    # Create one event per index position and append to a list for that row\n",
    "    events = []\n",
    "    for i in range(max_len):\n",
    "        event = {\n",
    "            \"agent\": prepared[\"agent\"][i] if not pd.isna(prepared[\"agent\"][i]) else None,\n",
    "            \"agent_norm\": prepared[\"agent_norm\"][i] if not pd.isna(prepared[\"agent_norm\"][i]) else None,\n",
    "            \"action_or_event\": prepared[\"action_or_event\"][i] if not pd.isna(prepared[\"action_or_event\"][i]) else None,\n",
    "            \"action_or_event_norm\": prepared[\"action_or_event_norm\"][i] if not pd.isna(prepared[\"action_or_event_norm\"][i]) else None,\n",
    "            \"object\": prepared[\"object\"][i] if not pd.isna(prepared[\"object\"][i]) else None,\n",
    "            \"object_norm\": prepared[\"object_norm\"][i] if not pd.isna(prepared[\"object_norm\"][i]) else None,\n",
    "            \"narrative\": prepared[\"narrative\"][i] if not pd.isna(prepared[\"narrative\"][i]) else None,\n",
    "            \"sentiment\": prepared[\"sentiment\"][i] if not pd.isna(prepared[\"sentiment\"][i]) else None,\n",
    "        }\n",
    "        events.append(event)\n",
    "    \n",
    "    fewshot_outputs_bluesky.append(events)\n",
    "fewshot_outputs_bluesky[2] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at structured data extraction and narrative understanding from social media data, specializing in the 2025 Canadian Presidential election. You will be given unstructured text from a social media post and should convert it into the given structure, a list of events where each event contains the following elements, focusing on figures in Canadian (and related international) politics:\n",
      "\n",
      "agent: One who is/has done the event.\n",
      "agent_norm: Normalized form of agent.\n",
      "action_or_event: Action which the agent has taken.\n",
      "action_or_event_norm: Normalized form of action_or_event.\n",
      "object: One who is receiving the action or being acted upon.\n",
      "object_norm: Normalized form of object.\n",
      "narrative: Short, 1-sentence description of the larger narrative that this agent-action-object triple seems to be a part of.\n",
      "\n",
      "A post may contain no events or multiple. Extract all identified events in the post. Elements may be explicitly found in the post or implicit. Elements that cannot be filled should be left as None. If the social media post's poster is extracted as an element, they should be referred to as \"User\". All other people can be identifed by their name and social media handle (if found in the post).\n"
     ]
    }
   ],
   "source": [
    "zeroshot_system_prompt = '''\n",
    "You are an expert at structured data extraction and narrative understanding from social media data, specializing in the 2025 Canadian Presidential election. You will be given unstructured text from a social media post and should convert it into the given structure, a list of events where each event contains the following elements, focusing on figures in Canadian (and related international) politics:\n",
    "\n",
    "agent: One who is/has done the event.\n",
    "agent_norm: Normalized form of agent.\n",
    "action_or_event: Action which the agent has taken.\n",
    "action_or_event_norm: Normalized form of action_or_event.\n",
    "object: One who is receiving the action or being acted upon.\n",
    "object_norm: Normalized form of object.\n",
    "narrative: Short, 1-sentence description of the larger narrative that this agent-action-object triple seems to be a part of.\n",
    "\n",
    "A post may contain no events or multiple. Extract all identified events in the post. Elements may be explicitly found in the post or implicit. Elements that cannot be filled should be left as None. If the social media post's poster is extracted as an element, they should be referred to as \"User\". All other people can be identifed by their name and social media handle (if found in the post).\n",
    "'''.strip()\n",
    "print(zeroshot_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at structured data extraction and narrative understanding from social media data, specializing in the 2025 Canadian Presidential election. You will be given unstructured text from a social media post and should convert it into the given structure, a list of events where each event contains the following elements, focusing on figures in Canadian (and related international) politics:\n",
      "\n",
      "agent: One who is/has done the event.\n",
      "agent_norm: Normalized form of agent.\n",
      "action_or_event: Action which the agent has taken.\n",
      "action_or_event_norm: Normalized form of action_or_event.\n",
      "object: One who is receiving the action or being acted upon.\n",
      "object_norm: Normalized form of object.\n",
      "narrative: Short, 1-sentence description of the larger narrative that this agent-action-object triple seems to be a part of.\n",
      "\n",
      "A post may contain no events or multiple. Extract all identified events in the post. Elements may be explicitly found in the post or implicit. Elements that cannot be filled should be left as None. If the social media post's poster is extracted as an element, they should be referred to as \"User\". All other people can be identifed by their name and social media handle (if found in the post).\n",
      "\n",
      "Here are some examples of valid extractions:\n"
     ]
    }
   ],
   "source": [
    "fewshot_system_prompt = '''\n",
    "You are an expert at structured data extraction and narrative understanding from social media data, specializing in the 2025 Canadian Presidential election. You will be given unstructured text from a social media post and should convert it into the given structure, a list of events where each event contains the following elements, focusing on figures in Canadian (and related international) politics:\n",
    "\n",
    "agent: One who is/has done the event.\n",
    "agent_norm: Normalized form of agent.\n",
    "action_or_event: Action which the agent has taken.\n",
    "action_or_event_norm: Normalized form of action_or_event.\n",
    "object: One who is receiving the action or being acted upon.\n",
    "object_norm: Normalized form of object.\n",
    "narrative: Short, 1-sentence description of the larger narrative that this agent-action-object triple seems to be a part of.\n",
    "\n",
    "A post may contain no events or multiple. Extract all identified events in the post. Elements may be explicitly found in the post or implicit. Elements that cannot be filled should be left as None. If the social media post's poster is extracted as an element, they should be referred to as \"User\". All other people can be identifed by their name and social media handle (if found in the post).\n",
    "\n",
    "Here are some examples of valid extractions:\n",
    "'''.strip()\n",
    "print(fewshot_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bluesky-specific few-shot prompt\n",
    "fewshot_system_prompt_bluesky = f'''\n",
    "{fewshot_system_prompt}\n",
    "\n",
    "Example Input 1:\n",
    "{fewshot_inputs_bluesky[0]}\n",
    "Example Output 1:\n",
    "{fewshot_outputs_bluesky[0]}\n",
    "\n",
    "Example Input 2:\n",
    "{fewshot_inputs_bluesky[1]}\n",
    "Example Output 2:\n",
    "{fewshot_outputs_bluesky[1]}\n",
    "\n",
    "Example Input 3:\n",
    "{fewshot_inputs_bluesky[2]}\n",
    "Example Output 3:\n",
    "{fewshot_outputs_bluesky[2]}\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-specific few-shot prompt\n",
    "fewshot_system_prompt_x = f'''\n",
    "{fewshot_system_prompt}\n",
    "\n",
    "Example Input 1:\n",
    "{fewshot_inputs_x[0]}\n",
    "Example Output 1:\n",
    "{fewshot_outputs_x[0]}\n",
    "\n",
    "Example Input 2:\n",
    "{fewshot_inputs_x[1]}\n",
    "Example Output 2:\n",
    "{fewshot_outputs_x[1]}\n",
    "\n",
    "Example Input 3:\n",
    "{fewshot_inputs_x[2]}\n",
    "Example Output 3:\n",
    "{fewshot_outputs_x[2]}\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "Based on evaluation on annotations:\n",
    "- Bluesky: 0-shot\n",
    "- X: fewshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost estimation\n",
    "Estimate cost based on input prompts over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_per_input_token  = 0.15 / 1_000_000\n",
    "price_per_output_token = 0.60 / 1_000_000\n",
    "\n",
    "enc = tiktoken.encoding_for_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_in_messages(messages):\n",
    "    total = 0\n",
    "    for msg in messages:\n",
    "        total += len(enc.encode(msg[\"role\"]))\n",
    "        total += len(enc.encode(msg[\"content\"]))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_chat_batch_cost(batched_messages, responses):\n",
    "    cost = 0.0\n",
    "    for msgs, reply in zip(batched_messages, responses):\n",
    "        in_toks  = count_tokens_in_messages(msgs)\n",
    "        out_toks = len(enc.encode(reply))\n",
    "        cost += in_toks * price_per_input_token + out_toks * price_per_output_token\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bluesky prompts\n",
    "# prompts = []\n",
    "# for text in posts_bluesky['record_text']:\n",
    "#     prompts.append([\n",
    "#         {'role': 'system', 'content': zeroshot_system_prompt},\n",
    "#         {'role': 'user', 'content': text}\n",
    "#     ])\n",
    "# cost = estimate_chat_batch_cost(prompts, ['']*len(prompts))\n",
    "# print('Estimated cost:', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X prompts\n",
    "# prompts = []\n",
    "# for text in posts_x['full_text']:\n",
    "#     prompts.append([\n",
    "#         {'role': 'system', 'content': fewshot_system_prompt_x},\n",
    "#         {'role': 'user', 'content': text}\n",
    "#     ])\n",
    "# cost = estimate_chat_batch_cost(prompts, ['']*len(prompts))\n",
    "# print('Estimated cost:', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full datasets take about $80 just for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23872, 27) (29166, 34)\n"
     ]
    }
   ],
   "source": [
    "# Just operate on first 2 weeks of March\n",
    "posts_bluesky_filt = posts_bluesky[posts_bluesky['first_updated'] < '2025-03-08']\\\n",
    "    .drop_duplicates(['uri'], keep='last')\n",
    "posts_x_filt = posts_x[posts_x['date_collected'] < '2025-03-08']\\\n",
    "    .drop_duplicates(['tweet_id'], keep='last')\n",
    "\n",
    "print(posts_bluesky_filt.shape, posts_x_filt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bluesky prompts\n",
    "# prompts = []\n",
    "# for text in posts_bluesky_filt['record_text']:\n",
    "#     prompts.append([\n",
    "#         {'role': 'system', 'content': zeroshot_system_prompt},\n",
    "#         {'role': 'user', 'content': text}\n",
    "#     ])\n",
    "# cost = estimate_chat_batch_cost(prompts, ['']*len(prompts))\n",
    "# print('Estimated cost:', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X prompts\n",
    "# prompts = []\n",
    "# for text in posts_x_filt['full_text']:\n",
    "#     prompts.append([\n",
    "#         {'role': 'system', 'content': fewshot_system_prompt_x},\n",
    "#         {'role': 'user', 'content': text}\n",
    "#     ])\n",
    "# cost = estimate_chat_batch_cost(prompts, ['']*len(prompts))\n",
    "# print('Estimated cost:', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtered datasets take about $14 for input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bluesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Final export to: gpt4omini_posts_bluesky_20250301_20250314_extractions_zeroshot\n"
     ]
    }
   ],
   "source": [
    "processed_data_dir = os.path.join('./data', 'processed')\n",
    "fname = 'gpt4omini_posts_bluesky_20250301_20250314_extractions_zeroshot'\n",
    "fname_parsed_rounds_in_prog = os.path.join(\n",
    "    processed_data_dir, f'in_prog_{fname}.pkl'\n",
    ")\n",
    "save_every = 500\n",
    "\n",
    "# Use uri as id\n",
    "id2processed_text = {}\n",
    "\n",
    "# Whether to load existing\n",
    "resume_existing = True\n",
    "if resume_existing:\n",
    "    if os.path.exists(fname_parsed_rounds_in_prog):\n",
    "        with open(fname_parsed_rounds_in_prog, 'rb') as f:\n",
    "            id2processed_text = pickle.load(f)\n",
    "\n",
    "for idx, row in enumerate(posts_bluesky_filt.iterrows()):\n",
    "    row = row[1]\n",
    "    id_ = row['uri']\n",
    "    if id_ in id2processed_text:\n",
    "        continue\n",
    "    try:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": zeroshot_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": row['record_text']}\n",
    "            ],\n",
    "            response_format=NarrativeExtraction,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        try:\n",
    "            parsed_round = message.content\n",
    "        except:\n",
    "            # Parsing error\n",
    "            print(f'error_parsing round: {round_n}')\n",
    "            parsed_round = message\n",
    "    except:\n",
    "        # Invalid JSON\n",
    "        parsed_round = {}\n",
    "    id2processed_text[id_] = parsed_round\n",
    "\n",
    "    # Save intermittently\n",
    "    if (idx + 1) % save_every == 0:\n",
    "        with open(fname_parsed_rounds_in_prog, 'wb') as f:\n",
    "            pickle.dump(id2processed_text, f)\n",
    "        print(f'# processed: {idx + 1}')\n",
    "print('DONE')\n",
    "\n",
    "processed_data_dir = os.path.join('./data', 'processed')\n",
    "fname_parsed_rounds = os.path.join(processed_data_dir, f'{fname}.pkl')\n",
    "\n",
    "with open(fname_parsed_rounds, 'wb') as f:\n",
    "    pickle.dump(id2processed_text, f)\n",
    "print('Final Bluesky export to:', fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Final export to: gpt4omini_posts_x_20250301_20250314_extractions_zeroshot\n"
     ]
    }
   ],
   "source": [
    "processed_data_dir = os.path.join('./data', 'processed')\n",
    "fname = 'gpt4omini_posts_x_20250301_20250314_extractions_zeroshot'\n",
    "fname_parsed_rounds_in_prog = os.path.join(\n",
    "    processed_data_dir, f'in_prog_{fname}.pkl'\n",
    ")\n",
    "save_every = 500\n",
    "\n",
    "# Use tweet_id as id\n",
    "id2processed_text = {}\n",
    "\n",
    "# Whether to load existing\n",
    "resume_existing = True\n",
    "if resume_existing:\n",
    "    if os.path.exists(fname_parsed_rounds_in_prog):\n",
    "        with open(fname_parsed_rounds_in_prog, 'rb') as f:\n",
    "            id2processed_text = pickle.load(f)\n",
    "\n",
    "for idx, row in enumerate(posts_x_filt.iterrows()):\n",
    "    row = row[1]\n",
    "    id_ = row['tweet_id']\n",
    "    if id_ in id2processed_text:\n",
    "        continue\n",
    "    try:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": fewshot_system_prompt_x},\n",
    "                {\"role\": \"user\", \"content\": row['full_text']}\n",
    "            ],\n",
    "            response_format=NarrativeExtraction,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        try:\n",
    "            parsed_round = message.content\n",
    "        except:\n",
    "            # Parsing error\n",
    "            print(f'error_parsing round: {round_n}')\n",
    "            parsed_round = message\n",
    "    except:\n",
    "        # Invalid JSON\n",
    "        parsed_round = {}\n",
    "    id2processed_text[id_] = parsed_round\n",
    "\n",
    "    # Save intermittently\n",
    "    if (idx + 1) % save_every == 0:\n",
    "        with open(fname_parsed_rounds_in_prog, 'wb') as f:\n",
    "            pickle.dump(id2processed_text, f)\n",
    "        print(f'# processed: {idx + 1}')\n",
    "print('DONE')\n",
    "\n",
    "processed_data_dir = os.path.join('./data', 'processed')\n",
    "fname_parsed_rounds = os.path.join(processed_data_dir, f'{fname}.pkl')\n",
    "\n",
    "with open(fname_parsed_rounds, 'wb') as f:\n",
    "    pickle.dump(id2processed_text, f)\n",
    "print('Final X export to:', fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from: /nas/ckgfs/users/eboxer/complexdata/data/processed/in_prog_gpt4omini_posts_bluesky_20250301_20250314_extractions_zeroshot.pkl\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Peek on the results\n",
    "fname_parsed_rounds = '/nas/ckgfs/users/eboxer/complexdata/data/processed/in_prog_gpt4omini_posts_bluesky_20250301_20250314_extractions_zeroshot.pkl'\n",
    "with open(fname_parsed_rounds, 'rb') as f:\n",
    "    id2processed_text = pickle.load(f)\n",
    "# id2processed_text = {id_: json.loads(text)['events'] for id_, text in id2processed_text.items()}\n",
    "print('Read from:', fname_parsed_rounds)\n",
    "print(len(id2processed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on runs. Expect a significant amount of progress on bluesky at least.\n",
    "\n",
    "If not, interrupt, write some code to pick up from the last checkpoint (by input id) and add more filtering (just a week or less per dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexdata-kernel",
   "language": "python",
   "name": "complexdata-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
